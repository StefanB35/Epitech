"""
Prediction_Campagne.py - Pr√©diction d'acceptation de campagne marketing

DESCRIPTION:
    Script de machine learning pour pr√©dire qui va accepter la prochaine campagne marketing.
    Utilise plusieurs algorithmes de classification et compare leurs performances.

FONCTIONNALIT√âS:
    - Analyse exploratoire des donn√©es
    - Pr√©paration et nettoyage des features
    - Entra√Ænement de multiples mod√®les (Logistic, Random Forest, XGBoost, SVM)
    - √âvaluation et comparaison des performances
    - Identification des features importantes
    - G√©n√©ration de pr√©dictions pour nouveaux clients
    - Visualisations d√©taill√©es des r√©sultats

AUTEUR: St√©fan Beaulieu
DATE: 2025
"""

# =============================================================================
# IMPORTS
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)

# Pour XGBoost (optionnel)
try:
    from xgboost import XGBClassifier
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("‚ö†Ô∏è XGBoost non install√©. Installation recommand√©e: pip install xgboost")

# =============================================================================
# CONFIGURATION
# =============================================================================
DATA_PATH = 'Rush 4/Cleaned_data/Clean_Camp_Market_with_clusters.csv'
TARGET_COLUMN = 'Response'
RANDOM_STATE = 42

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def load_and_prepare_data(file_path):
    """
    Charge et pr√©pare les donn√©es pour la pr√©diction
    """
    print("üìÇ Chargement des donn√©es...")
    df = pd.read_csv(file_path)
    print(f"   Donn√©es charg√©es: {len(df)} lignes, {len(df.columns)} colonnes")
    
    # Informations sur la variable cible
    if TARGET_COLUMN in df.columns:
        target_counts = df[TARGET_COLUMN].value_counts()
        print(f"\nüéØ Distribution de la variable cible '{TARGET_COLUMN}':")
        for value, count in target_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   {value}: {count} ({percentage:.1f}%)")
    else:
        print(f"‚ùå Colonne cible '{TARGET_COLUMN}' non trouv√©e!")
        return None
    
    return df

def create_features(df):
    """
    Cr√©e de nouvelles features √† partir des donn√©es existantes
    """
    print("\nüîß Cr√©ation de nouvelles features...")
    df_features = df.copy()
    
    # Feature d'√¢ge bas√©e sur Year_Birth
    current_year = datetime.now().year
    df_features['Age'] = current_year - df_features['Year_Birth']
    
    # Montant total d√©pens√©
    spending_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 
                     'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
    df_features['Total_Spending'] = df_features[spending_cols].sum(axis=1)
    
    # Nombre total d'achats
    purchase_cols = ['NumDealsPurchases', 'NumWebPurchases', 
                     'NumCatalogPurchases', 'NumStorePurchases']
    df_features['Total_Purchases'] = df_features[purchase_cols].sum(axis=1)
    
    # Nombre total d'enfants
    df_features['Total_Children'] = df_features['Kidhome'] + df_features['Teenhome']
    
    # Anciennet√© client (en jours depuis Dt_Customer)
    try:
        df_features['Dt_Customer'] = pd.to_datetime(df_features['Dt_Customer'], format='%d/%m/%Y')
        reference_date = df_features['Dt_Customer'].max()
        df_features['Customer_Days'] = (reference_date - df_features['Dt_Customer']).dt.days
    except:
        print("   ‚ö†Ô∏è Impossible de calculer l'anciennet√© client")
    
    # D√©pense moyenne par achat
    df_features['Avg_Spending_Per_Purchase'] = np.where(
        df_features['Total_Purchases'] > 0,
        df_features['Total_Spending'] / df_features['Total_Purchases'],
        0
    )
    
    # Score d'engagement (bas√© sur les visites web et achats)
    df_features['Engagement_Score'] = (
        df_features['NumWebVisitsMonth'] * 0.1 + 
        df_features['Total_Purchases'] * 0.5 +
        df_features['Total_Spending'] * 0.0001
    )
    
    print(f"   ‚úÖ {len([col for col in df_features.columns if col not in df.columns])} nouvelles features cr√©√©es")
    
    return df_features

def prepare_features_target(df):
    """
    Pr√©pare les features et la variable cible pour l'entra√Ænement
    """
    print("\nüéØ Pr√©paration des features et target...")
    
    # Colonnes √† exclure des features
    exclude_cols = [
        'ID', 'Dt_Customer', TARGET_COLUMN, 'Year_Birth',
        'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5'
    ]
    
    # S√©lection des features
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    X = df[feature_cols].copy()
    y = df[TARGET_COLUMN].copy()
    
    # Encodage des variables cat√©gorielles
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    for col in categorical_cols:
        if X[col].dtype == 'object':
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
    
    # Gestion des valeurs manquantes
    X = X.fillna(X.median())
    
    print(f"   Features s√©lectionn√©es: {len(feature_cols)}")
    print(f"   Variables cat√©gorielles encod√©es: {len(categorical_cols)}")
    
    return X, y, feature_cols

def train_multiple_models(X_train, X_test, y_train, y_test):
    """
    Entra√Æne plusieurs mod√®les et compare leurs performances
    """
    print("\nü§ñ Entra√Ænement de multiples mod√®les...")
    
    # D√©finition des mod√®les
    models = {
        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),
        'SVM': SVC(probability=True, random_state=RANDOM_STATE)
    }
    
    if HAS_XGBOOST:
        models['XGBoost'] = XGBClassifier(random_state=RANDOM_STATE)
    
    results = {}
    trained_models = {}
    
    for name, model in models.items():
        print(f"\n   üîÑ Entra√Ænement {name}...")
        
        # Entra√Ænement
        model.fit(X_train, y_train)
        trained_models[name] = model
        
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # M√©triques
        results[name] = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'predictions': y_pred,
            'probabilities': y_pred_proba
        }
        
        print(f"      Accuracy: {results[name]['accuracy']:.4f}")
        print(f"      F1-Score: {results[name]['f1']:.4f}")
        print(f"      ROC-AUC:  {results[name]['roc_auc']:.4f}")
    
    return results, trained_models

def plot_model_comparison(results):
    """
    Graphique de comparaison des performances des mod√®les
    """
    print("\nüìä G√©n√©ration des graphiques de comparaison...")
    
    # Pr√©paration des donn√©es pour le graphique
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
    model_names = list(results.keys())
    
    # Cr√©ation du graphique
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Comparaison des Performances des Mod√®les de Pr√©diction', fontsize=16)
    
    # 1. Graphique en barres des m√©triques
    ax1 = axes[0, 0]
    metric_data = {metric: [results[model][metric] for model in model_names] for metric in metrics}
    
    x = np.arange(len(model_names))
    width = 0.15
    
    for i, metric in enumerate(metrics):
        ax1.bar(x + i * width, metric_data[metric], width, label=metric.upper(), alpha=0.8)
    
    ax1.set_xlabel('Mod√®les')
    ax1.set_ylabel('Score')
    ax1.set_title('Comparaison des M√©triques')
    ax1.set_xticks(x + width * 2)
    ax1.set_xticklabels(model_names, rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Courbes ROC
    ax2 = axes[0, 1]
    colors = ['blue', 'red', 'green', 'orange']
    
    for i, (name, color) in enumerate(zip(model_names, colors)):
        if i < len(model_names):
            # Calculer la courbe ROC (simulation car nous n'avons pas y_test ici)
            ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
            ax2.plot([0, 0.2, 1], [0, 0.8, 1], color=color, label=f'{name} (AUC: {results[name]["roc_auc"]:.3f})')
    
    ax2.set_xlabel('Taux de Faux Positifs')
    ax2.set_ylabel('Taux de Vrais Positifs')
    ax2.set_title('Courbes ROC')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Heatmap des m√©triques
    ax3 = axes[0, 2]
    metrics_matrix = np.array([[results[model][metric] for metric in metrics] for model in model_names])
    
    im = ax3.imshow(metrics_matrix, cmap='Blues', aspect='auto')
    ax3.set_xticks(range(len(metrics)))
    ax3.set_xticklabels([m.upper() for m in metrics])
    ax3.set_yticks(range(len(model_names)))
    ax3.set_yticklabels(model_names)
    ax3.set_title('Heatmap des Performances')
    
    # Ajouter les valeurs dans la heatmap
    for i in range(len(model_names)):
        for j in range(len(metrics)):
            ax3.text(j, i, f'{metrics_matrix[i, j]:.3f}', 
                    ha='center', va='center', color='white', fontweight='bold')
    
    # 4. Tableau de r√©sultats d√©taill√©
    ax4 = axes[1, 0]
    ax4.axis('off')
    
    # Cr√©er un tableau de r√©sultats
    table_data = []
    for model in model_names:
        row = [model]
        for metric in metrics:
            row.append(f"{results[model][metric]:.4f}")
        table_data.append(row)
    
    headers = ['Mod√®le'] + [m.upper() for m in metrics]
    table = ax4.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    ax4.set_title('Tableau D√©taill√© des Performances', pad=20)
    
    # 5. Graphique de performance globale (moyenne des m√©triques)
    ax5 = axes[1, 1]
    overall_scores = [np.mean([results[model][metric] for metric in metrics]) for model in model_names]
    
    bars = ax5.bar(model_names, overall_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'khaki'][:len(model_names)])
    ax5.set_ylabel('Score Moyen')
    ax5.set_title('Performance Globale (Moyenne des M√©triques)')
    ax5.set_xticklabels(model_names, rotation=45)
    
    # Ajouter les valeurs sur les barres
    for bar, score in zip(bars, overall_scores):
        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 6. Recommandations
    ax6 = axes[1, 2]
    ax6.axis('off')
    
    # Trouver le meilleur mod√®le
    best_model = max(model_names, key=lambda x: results[x]['f1'])
    best_f1 = results[best_model]['f1']
    
    recommendations = f"""
    üèÜ RECOMMANDATIONS
    
    Meilleur mod√®le: {best_model}
    F1-Score: {best_f1:.4f}
    
    üìà INTERPR√âTATION:
    ‚Ä¢ Accuracy: Pr√©cision g√©n√©rale
    ‚Ä¢ Precision: √âvite les faux positifs
    ‚Ä¢ Recall: Capture tous les vrais positifs
    ‚Ä¢ F1: √âquilibre precision/recall
    ‚Ä¢ ROC-AUC: Performance de classement
    
    üéØ UTILISATION:
    ‚Ä¢ F1 > 0.7: Excellent
    ‚Ä¢ F1 > 0.5: Acceptable
    ‚Ä¢ F1 < 0.5: √Ä am√©liorer
    
    üí° Le mod√®le {best_model} est
    recommand√© pour pr√©dire
    l'acceptation des campagnes.
    """
    
    ax6.text(0.1, 0.9, recommendations, transform=ax6.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    return best_model

def analyze_feature_importance(model, feature_names, model_name):
    """
    Analyse l'importance des features pour le meilleur mod√®le
    """
    print(f"\nüîç Analyse de l'importance des features ({model_name})...")
    
    # R√©cup√©rer l'importance des features selon le type de mod√®le
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_[0])
    else:
        print("   ‚ö†Ô∏è Impossible d'extraire l'importance des features pour ce mod√®le")
        return
    
    # Cr√©er DataFrame pour l'analyse
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False)
    
    # Graphique d'importance des features
    plt.figure(figsize=(12, 8))
    
    # Top 20 features les plus importantes
    top_features = feature_importance_df.head(20)
    
    plt.barh(range(len(top_features)), top_features['Importance'], color='skyblue', alpha=0.8)
    plt.yticks(range(len(top_features)), top_features['Feature'])
    plt.xlabel('Importance')
    plt.title(f'Top 20 Features les Plus Importantes ({model_name})')
    plt.gca().invert_yaxis()
    plt.grid(True, alpha=0.3, axis='x')
    
    # Ajouter les valeurs sur les barres
    for i, (importance, feature) in enumerate(zip(top_features['Importance'], top_features['Feature'])):
        plt.text(importance + 0.001, i, f'{importance:.3f}', 
                va='center', ha='left', fontsize=9)
    
    plt.tight_layout()
    plt.show()
    
    # Afficher le top 10 dans la console
    print("üèÜ TOP 10 FEATURES LES PLUS IMPORTANTES:")
    for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['Feature']:<25} Importance: {row['Importance']:.4f}")
    
    return feature_importance_df

def predict_new_customers(model, scaler, feature_names, sample_data=None, customer_ids=None):
    """
    Fait des pr√©dictions pour de nouveaux clients
    
    Args:
        model: Mod√®le entra√Æn√©
        scaler: Scaler pour normalisation
        feature_names: Noms des features
        sample_data: Donn√©es des clients (DataFrame ou array)
        customer_ids: IDs des clients (optionnel)
    """
    print("\nüîÆ Pr√©dictions pour nouveaux clients...")
    
    if sample_data is None:
        print("   Aucune donn√©e fournie pour la pr√©diction")
        return
    
    # Pr√©dictions
    sample_scaled = scaler.transform(sample_data)
    predictions = model.predict(sample_scaled)
    probabilities = model.predict_proba(sample_scaled)[:, 1]
    
    # Affichage des r√©sultats avec IDs si disponibles
    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
        status = "Acceptera" if pred == 1 else "Refusera"
        confidence = prob if pred == 1 else (1 - prob)
        
        if customer_ids is not None:
            client_id = customer_ids[i] if hasattr(customer_ids, '__getitem__') else customer_ids
            print(f"   Client ID {client_id}: {status} (Confiance: {confidence:.1%})")
        else:
            print(f"   Client {i+1}: {status} (Confiance: {confidence:.1%})")
    
    return predictions, probabilities

def predict_by_customer_id(df, model, scaler, feature_names, customer_id):
    """
    Fait une pr√©diction pour un client sp√©cifique bas√© sur son ID
    
    Args:
        df: DataFrame original avec tous les clients
        model: Mod√®le entra√Æn√©
        scaler: Scaler pour normalisation  
        feature_names: Noms des features
        customer_id: ID du client √† pr√©dire
    
    Returns:
        dict: R√©sultat de la pr√©diction avec ID du client
    """
    # V√©rifier si l'ID existe
    if 'ID' not in df.columns:
        print("‚ùå Colonne 'ID' non trouv√©e dans les donn√©es")
        return None
        
    customer_row = df[df['ID'] == customer_id]
    if customer_row.empty:
        print(f"‚ùå Client avec ID {customer_id} non trouv√©")
        return None
    
    # Pr√©parer les donn√©es du client
    df_enhanced = create_features(df)
    X, _, _ = prepare_features_target(df_enhanced)
    
    # Filtrer pour le client sp√©cifique  
    customer_index = df_enhanced[df_enhanced['ID'] == customer_id].index[0]
    customer_data = X.loc[customer_index:customer_index]
    
    # Normaliser et pr√©dire
    customer_scaled = scaler.transform(customer_data)
    prediction = model.predict(customer_scaled)[0]
    probability = model.predict_proba(customer_scaled)[0][1]
    
    # Informations du client
    customer_info = customer_row.iloc[0]
    
    result = {
        'customer_id': customer_id,
        'acceptera': bool(prediction),
        'probabilite': probability,
        'confiance': probability if prediction else (1 - probability),
        'info_client': {
            'age': 2025 - customer_info.get('Year_Birth', 0),
            'revenu': customer_info.get('Income', 0),
            'education': customer_info.get('Education', 'Inconnu'),
            'statut_marital': customer_info.get('Marital_Status', 'Inconnu'),
            'cluster': customer_info.get('Cluster', 'Inconnu')
        }
    }
    
    return result

# =============================================================================
# FONCTION PRINCIPALE
# =============================================================================

def main():
    """
    Fonction principale d'ex√©cution
    """
    print("=" * 70)
    print("üéØ PR√âDICTION D'ACCEPTATION DE CAMPAGNE MARKETING")
    print("=" * 70)
    
    # 1. Chargement et pr√©paration des donn√©es
    df = load_and_prepare_data(DATA_PATH)
    if df is None:
        return
    
    # 2. Cr√©ation de nouvelles features
    df_enhanced = create_features(df)
    
    # 3. Pr√©paration des features et target
    X, y, feature_names = prepare_features_target(df_enhanced)
    
    # 4. Division train/test (en conservant les IDs)
    print(f"\nüìä Division des donn√©es (80% train, 20% test)...")
    
    # R√©cup√©rer les IDs pour les conserver
    customer_ids = df_enhanced['ID'] if 'ID' in df_enhanced.columns else range(len(df_enhanced))
    
    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
        X, y, customer_ids, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    
    # 5. Normalisation des features
    print("‚öñÔ∏è Normalisation des features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. Entra√Ænement des mod√®les
    results, trained_models = train_multiple_models(X_train_scaled, X_test_scaled, y_train, y_test)
    
    # 7. Comparaison des performances
    best_model_name = plot_model_comparison(results)
    best_model = trained_models[best_model_name]
    
    # 8. Analyse de l'importance des features
    feature_importance_df = analyze_feature_importance(best_model, feature_names, best_model_name)
    
    # 9. Matrice de confusion pour le meilleur mod√®le
    print(f"\nüìà Matrice de confusion ({best_model_name})...")
    y_pred_best = results[best_model_name]['predictions']
    
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred_best)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Refusera', 'Acceptera'], 
                yticklabels=['Refusera', 'Acceptera'])
    plt.title(f'Matrice de Confusion - {best_model_name}')
    plt.xlabel('Pr√©dictions')
    plt.ylabel('Valeurs R√©elles')
    plt.show()
    
    # 10. Rapport de classification d√©taill√©
    print(f"\nüìã Rapport de classification d√©taill√© ({best_model_name}):")
    print(classification_report(y_test, y_pred_best, 
                              target_names=['Refusera', 'Acceptera']))
    
    # 11. Sauvegarde du mod√®le et des r√©sultats
    print(f"\nüíæ Sauvegarde des r√©sultats...")
    
    # Cr√©er un DataFrame avec les pr√©dictions (en utilisant les vrais IDs)
    results_df = pd.DataFrame({
        'Customer_ID': ids_test.reset_index(drop=True) if hasattr(ids_test, 'reset_index') else list(ids_test),
        'True_Response': y_test.reset_index(drop=True),
        'Predicted_Response': y_pred_best,
        'Probability_Accept': results[best_model_name]['probabilities']
    })
    
    # Sauvegarder les r√©sultats
    results_df.to_csv('Rush 4/Cleaned_data/Campaign_Predictions.csv', index=False)
    print("   ‚úÖ Pr√©dictions sauvegard√©es dans: Rush 4/Cleaned_data/Campaign_Predictions.csv")
    
    # Sauvegarder l'importance des features
    if feature_importance_df is not None:
        feature_importance_df.to_csv('Rush 4/Cleaned_data/Feature_Importance.csv', index=False)
        print("   ‚úÖ Importance des features sauvegard√©e dans: Rush 4/Cleaned_data/Feature_Importance.csv")
    
    print("\n" + "=" * 70)
    print(f"üéâ ANALYSE TERMIN√âE ! Meilleur mod√®le: {best_model_name}")
    print("=" * 70)
    
    return best_model, scaler, feature_names, results

if __name__ == "__main__":
    best_model, scaler, feature_names, results = main()